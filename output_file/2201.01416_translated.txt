

==================================== 第1页 ====================================

Latent Vector Expansion using Autoencoder for Anomaly Detection UJu Gim∗, YeongHyeon Park SK Planet Co., Ltd. Keywords: Autoencoder, Anomaly Detection, Deep Learn- ing Abstract Deep learning methods can classify various unstructured data such as images, language, and voice as input data. As the task of classifying anomalies becomes more important in the real world, various methods exist for classifying using deep learn- ing with data collected in the real world. As the task of clas- sifying anomalies becomes more important in the real world, there are various methods for classifying using deep learning with data collected in the real world. Among the various meth- ods, the representative approach is a method of extracting and learning the main features based on a transition model from pre-trained models, and a method of learning an autoencoder- based structure only with normal data and classifying it as ab- normal through a threshold value. However, if the dataset is imbalanced, even the state-of-the-arts models do not achieve good performance. This can be addressed by augmenting nor- mal and abnormal features in imbalanced data as features with strong distinction. We use the features of the autoencoder to train latent vectors from low to high dimensionality. We train normal and abnormal data as a feature that has a strong distinc- tion among the features of imbalanced data. We propose a la- tent vector expansion autoencoder model that improves classi- fication performance at imbalanced data. The proposed method shows performance improvement compared to the basic au- toencoder using imbalanced anomaly dataset. 1 Introduction Nowadays, using deep learning through data sets collected in the real world, image classification [1, 2] and abnormal situa- tion determination [3, 4] among various unstructured data are in progress. In order to classify images and determine abnor- mal situations, data is collected and meaningful feature values are extracted from the collected datasets. The extracted signif- icant feature values enable the machine to recognize the input data of the data set. However, there is a problem that the ma- chine does not recognize the difference between normal and abnormal even if data pre-processing is performed among the collected data, and it is difficult to compare the differences be- tween classes. To overcome this limitation, we train an autoen- coder with a simple change in the structure of the basic autoen- coder [5]. Normal and abnormal data are augmented through ∗Correspondence author: gim.uju1217@sk.com the latent vector generated by the encoder from trained autoen- coder. In order to extract strong feature values for normal and abnormal classification of the generated latent vectors, the low dimensionality is extended to the high dimensionality. This improves classification performance by strongly demarcating the decision boundaries of each class. We show an increase in performance over the basic autoencoder by experimenting with imbalanced anomaly dataset. 2 Related work The basic autoencoder consists of an encoder and a decoder. By reducing the dimension of the input value through the en- coder, the value is compressed into features representative of the input value. After that, the reduced-dimensional latent vec- tor is put as the input value of the decoder and the vector is restored to generate a value similar to the input value of the encoder. The purpose of a basic autoencoder is to train mean- ingful data from a reduced-dimensional latent vector in the en- coder. Variational autoencoder, similar in structure to autoen- coder, expresses probability distribution in input data through latent vectors [6]. After that, data is generated through the de- coder. The difference between an autoencoder and a variational autoencoder is as follows. The autoencoder recovers the re- duced data by reducing the dimensions of the data. Conversely, a variational autoencoder is a generative model that generates similar data in a probability distribution. The reason we use the autoencoder is that it is used only for the purpose of reducing the dimension, not for the purpose of data generation. Contrary to the encoder of the autoencoder, the kernel trick is to extend the characteristics of data from low to high di- mensionality to distinguish the boundary determination of each class [7]. Support vector machine (SVM) [8] improves the clas- sification performance of input data from low-dimensional lin- ear models to high-dimensional models. The kernel trick does not actually expand the data, but rather computes the scalar product of the data through the extended properties. We solve the classification problem by explosively expanding the dimen- sionality of the latent vector of the autoencoder, which is dif- ficult to classify due to the reduced dimensionality of feature values in low dimensions. 3 Latent vector expansion-based autoencoder In this section, we present the latent vector extension-based au- toencoder model, which is the method proposed in this paper. The Basic Autoencoder (BA) in ?? shows the basic autoen- arXiv:2201.01416v1 [cs.CV] 5 Jan 2022 


基于自动编码器的潜在矢量展开用于异常检测UJu Gim∗, YeongHyeon Park SK Planet Co.，Ltd.关键词：自动编码器、异常检测、深度学习抽象深度学习方法可以将图像、语言和语音等各种非结构化数据分类为输入数据。随着对异常进行分类的任务在现实世界中变得越来越重要，有各种方法可以使用在现实世界中收集的数据进行深度学习进行分类。随着对异常进行分类的任务在现实世界中变得越来越重要，有各种方法可以利用在现实世界中收集的数据进行深度学习进行分类。在各种方法中，有代表性的方法是基于预先训练模型的过渡模型提取和学习主要特征的方法，以及仅使用正常数据学习基于自动编码的结构并通过阈值将其分类为异常的方法。然而，如果数据集不平衡，即使是最先进的模型也无法获得良好的性能。这可以通过将不平衡数据中的正常和异常特征增强为具有强区分性的特征来解决。我们利用自动编码器的特性从低维到高维训练潜在向量。我们将正常数据和异常数据训练为一种特征，这种特征在不平衡数据的特征之间有很强的区别。我们提出了一个潜在的向量扩展自动编码器模型，提高了在不平衡数据下的分类性能。与使用不平衡异常数据集的基本自动编码器相比，该方法的性能有所提高。1简介如今，通过对现实世界中收集的数据集进行深入学习，各种非结构化数据中的图像分类[1,2]和异常情况确定[3,4]正在进行中。为了对图像进行分类并确定异常情况，收集数据并从收集的数据集中提取有意义的特征值。提取的重要特征值使机器能够识别数据集的输入数据。然而，存在一个问题，即即使在收集的数据中执行数据预处理，机器也无法识别正常和异常之间的差异，并且很难比较类别之间的差异。为了克服这一限制，我们对基本自动编码器的结构进行了简单的更改，从而训练出一个自动编码器[5]。正常和异常数据通过∗通讯作者：gim。uju1217@sk.com编码器从经过训练的自动编码器生成的潜在向量。为了对生成的潜在向量进行正常和异常分类，提取强特征值，将低维扩展到高维。这通过严格划分每个类的决策边界来提高分类性能。通过对不平衡异常数据集的实验，我们展示了比基本自动编码器性能的提高。2相关工作基本自动编码器由编码器和解码器组成。通过编码器降低输入值的维数，该值被压缩为代表输入值的特征。然后，将降维的潜在向量作为解码器的输入值，并恢复该向量以生成与编码器的输入值相似的值。基本自动编码器的目的是从编码器中的降维潜在向量中训练有意义的数据。变分自动编码器在结构上与自动编码器类似，通过潜在向量表达输入数据中的概率分布[6]。然后，通过解码器生成数据。自动编码器和可变自动编码器的区别如下。自动编码器通过减少数据的维数来恢复减少的数据。相反，变分自动编码器是一种生成模型，它以概率分布生成相似的数据。我们使用自动编码器的原因是，它仅用于降低尺寸，而不是用于数据生成。与自动编码器的编码器相反，核心技巧是将数据的特征从低维扩展到高维，以区分每类的边界确定[7]。支持向量机（SVM）[8]提高了输入数据从低维线性模型到高维模型的分类性能。内核技巧实际上并不扩展数据，而是通过扩展属性计算数据的标量积。我们通过爆炸式地扩展自动编码器潜在向量的维数来解决分类问题，这是由于低维特征值的维数降低而难以分类的。3基于潜在向量扩展的自动编码器在这一部分，我们提出了基于潜在向量扩展的自动编码器模型，这是本文提出的方法。基本自动编码器（BA）在？？显示了2022年1月5日的基本autoenarXiv:2201.01416v1[cs.CV]


==================================== 第2页 ====================================

Figure 1. Comparison of autoencoder structure coder and our autoencoder, which is restored through the de- coder after reducing the dimension of the encoder. Each para- graph describes our autoencoder and Latent Vector Expansion Network. We present our autoencoders different from BA. First, we add the first linear layer to the encoder of the au- toencoder, which specifies the size of the input value data. The second layer is set as a linear layer that reduces in proportion to the size of the first layer, and the input and output values are the same. Add the ReLu activation function to the first and second layers. The third layer of the encoder sets the output value of the second layer as an input value and sets the in- put value of the first layer as an output value. Subsequently, the decoder reverses the parameters of the set layer of the en- coder, and then sets the activation function of the last layer to sigmoid. We adopted ours, which expands the output of the second layer based on the output of the third layer through ex- periments. Table 1 shows the outstanding performance of ours, which expands the output value in the third layer after reduc- ing the size of each input data compared to the latent vector generated through the BA structure. A description of the linear model is given in the next paragraph, Latent Vector Expansion Network. Table 1. Autoencoder comparison with AUROC. Method AUROC Linear model w/ BA 0.872 Linear model w/ Ours 0.951 Second, we construct a Latent Vector Expansion network that trains latent vectors from the trained autoencoder’s en- coder. This network simply consists of two linear layers and a linear model as an output. After setting the input value of the first layer as the output value of the encoder, the output value expands the dimension to 1,024. After that, the activation func- tion uses ReLU and the dropout is set to 0.5. In addition, a lin- ear layer is set to set the output to 1 for normal and abnormal binary classification, and the final value is output through a log sigmoid. We show the 1024-dimensional performance through Table 2. A detailed description of each table is provided in Chapter 4. Table 2. Expansion dimension comparison with AUROC. Method Expansion dimension AUROC Ours w/ expansion 128 0.969 256 0.968 512 0.969 1,024 0.970 4 Experiments In this chapter, we present the dataset and preprocessing used for the experiment of our proposed latent vector expansion- based autoencoder. After that, the parameter settings used in each of Tables 1, 2 and 3 will be described. The experiment used credit card data set, and the main purpose of this data set is to classify fraud. It is an imbalanced data set with 492 ab- normal out of a total of 284,807 data, accounting for only about 0.17% [9]. We performed Min-Max scaling to prevent overfit- ting of imbalanced dataset, and conduct experiments through K-Fold, which is cross validation technic. For all models used in the experiment, the epoch is 20, the learning rate is 0.001, the optimizer used Adam, and the loss used binary cross en- tropy. The difference is that the autoencoder uses Mean Square error for Epoch 50 and loss during training. First in Table 1, w/ BA and w/ Ours set the encoder latent vector as the input value of the linear model after learning the autoencoder. Table 2 conducts the experiment by changing the extension dimension of the first layer of the latent vector extension net- work. Table 3 shows that each model conducts an experiment through the input value of the K-fold training dataset without an autoencoder. A latent vector extension network is used as a linear model. The w/o expansion changed the expansion di- mension of the first layer of the linear model to 10, and the w/ expansion changed the expansion dimension to 1,024 and conducted the experiment. 4.1 Experiment result In this chapter, we explain the reasons for the performance dif- ference between Table 3 and Table 4. We show the BA model and our model performance using K-Fold in Table 4. Com- pared with Table 3, Table 4 has relatively not good performance compared to the linear models, but the input value of the lin- ear model is basic data without reducing the dimension of the data. Since our model is data extracted from the encoder of a trained autoencoder, there are two advantages. The first is that it can be lightweight when learning the model, and the second can be faster than the basic data when inference is performed. In the comparison of models in Table 4, it shows superior per- formance in all Folds compared to when the BA is used. For this reason, our proposed Latent vector expansion-based au- toencoder and latent vector expansion method serve to improve performance. Figure 2 shows the PCA analysis of 2 and 2 folds showing the best performance and 5 and 7 folds showing not good per- 


图1。比较了自动编码器结构编码器和我们的自动编码器，这是通过减少编码器的尺寸后通过解码器恢复。每一段都描述了我们的自动编码器和潜在向量扩展网络。我们展示了不同于BA的自动编码器。首先，我们将第一个线性层添加到自动编码器的编码器中，该编码器指定输入值数据的大小。第二层设置为线性层，该线性层与第一层的大小成比例减小，且输入和输出值相同。将ReLu激活功能添加到第一层和第二层。编码器的第三层将第二层的输出值设置为输入值，并将第一层的输入值设置为输出值。随后，解码器反转编码器设置层的参数，然后将最后一层的激活函数设置为sigmoid。我们采用了我们的方法，通过实验在第三层输出的基础上扩展了第二层的输出。表1显示了我们的出色性能，与通过BA结构生成的潜在向量相比，减少了每个输入数据的大小，从而扩大了第三层的输出值。线性模型的描述将在下一段“潜在向量扩展网络”中给出。表1。自动编码器与AUROC的比较。方法AUROC线性模型w/BA 0.872线性模型w/Ours 0.951秒，我们构建一个潜在向量扩展网络，从经过训练的自动编码器的编码器中训练潜在向量。该网络由两个线性层和一个作为输出的线性模型组成。将第一层的输入值设置为编码器的输出值后，输出值将维度扩展到1024。之后，激活功能使用ReLU，退出设置为0.5。此外，设置一个线性层，将正常和异常二元分类的输出设置为1，最终值通过对数S形图输出。我们通过表2展示了1024维的性能。第4章提供了每个表的详细说明。表2。与AUROC的扩展维度比较。方法扩展维度AUROC Ours w/Expansion 128 0.969 256 0.968 512 0.969 1024 0.970 4实验在本章中，我们给出了用于我们提出的基于潜在向量扩展的自动编码器实验的数据集和预处理。之后，将描述表1、表2和表3中使用的参数设置。实验使用了信用卡数据集，该数据集的主要目的是对欺诈行为进行分类。这是一个不平衡的数据集，在总共284807个数据中有492个异常，仅占约0.17%[9]。为了防止不平衡数据的过度拟合，我们进行了最小-最大缩放，并通过交叉验证技术K-Fold进行了实验。对于实验中使用的所有模型，历元为20，学习率为0.001，优化器使用Adam，损失使用二进制交叉熵。不同之处在于，自动编码器使用历元50的均方误差和训练期间的损耗。首先在表1中，w/BA和w/Ours在学习自动编码器后，将编码器潜在向量设置为线性模型的输入值。表2通过改变潜在向量扩展网络第一层的扩展维度来进行实验。表3显示，每个模型在没有自动编码器的情况下，通过K-fold训练数据集的输入值进行实验。潜在向量扩展网络被用作线性模型。w/o展开将线性模型第一层的展开维数更改为10，w/o展开将展开维数更改为1024，并进行了实验。4.1实验结果在本章中，我们解释了表3和表4之间性能差异的原因。我们在表4中使用K-Fold展示了BA模型和我们的模型性能。与表3相比，表4相对于线性模型的性能相对较差，但线性模型的输入值是基础数据，没有降低数据的维数。由于我们的模型是从经过训练的自动编码器的编码器中提取的数据，因此有两个优点。第一种是，在学习模型时，它可以是轻量级的，第二种是在进行推理时，它可以比基础数据更快。在表4中的模型比较中，与使用BA时相比，它在所有折叠中都表现出了优越的性能。因此，我们提出的基于潜在矢量展开的自动编码器和潜在矢量展开方法可以提高性能。图2显示了PCA分析，2倍和2倍显示了最佳性能，5倍和7倍显示了不好的性能


==================================== 第3页 ====================================

formance in the two models (left is ours, right is BA in Figure 2) in Table 4. It can be seen that models classifier even the data set with not good data distribution using an expansion latent vector. Table 3. Linear model Comparison of with out expansion and with expansion models. Fold Linear model Linear model w/o expansion w/ expansion AUROC AUROC 1 0.994 0.990 2 0.995 0.995 3 0.998 0.999 4 0.935 0.927 5 0.964 0.972 6 0.986 0.987 7 0.990 0.991 8 0.970 0.978 9 0.983 0.985 10 0.973 0.981 5 Conclusion In this paper, we proposed a latent vector extension-based au- toencoder model that can show that our methodology is su- perior to that of the basic autoencoder structure and encoder for abnormal situation data. Through experiments, the pro- posed model showed that the abnormal situation detection per- formance was superior to the existing model through the credit card dataset. We will future experiment with images and vari- ous data sets. Table 4. BA model and our model performance comparison. Fold BA Ours w/ expansion w/ expansion AUROC AUROC 1 0.939 0.957 2 0.943 0.969 3 0.884 0.930 4 0.851 0.901 5 0.864 0.898 6 0.919 0.945 7 0.790 0.920 8 0.923 0.954 9 0.884 0.910 10 0.847 0.935 [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient- based learning applied to document recognition,” Proceed- ings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. [2] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” 2009. Figure 2. BA model and our model PCA Comparison [3] Y. Park, W. S. Park, and Y. B. Kim, “Anomaly detection in particulate matter sensor using hypothesis pruning genera- tive adversarial network,” ETRI Journal, 2020. [4] W. Sultani, C. Chen, and M. Shah, “Real-world anomaly detection in surveillance videos,” in Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 6479–6488, 2018. [5] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learn- ing algorithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp. 1527–1554, 2006. [6] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [7] B. Scholkopf, “The kernel trick for distances,” Advances in neural information processing systems, pp. 301–307, 2001. [8] W. S. Noble, “What is a support vector machine?,” Nature biotechnology, vol. 24, no. 12, pp. 1565–1567, 2006. [9] Y.-A. Le Borgne and G. Bontempi, “Machine learning for credit card fraud detection-practical handbook,” ACM SIGKDD explorations newsletter, vol. 6, no. 1, pp. 1–6, 2004. 


表4中两个模型的性能（左边是我们的，右边是图2中的BA）。可以看出，即使数据分布不好的数据集，模型也使用扩展潜在向量进行分类。表3。线性模型比较无膨胀和有膨胀模型。折叠线性模型线性模型w/o膨胀w/膨胀AUROC AUROC 1 0.994 0.990 2 0.995 0.995 3 0.998 0.999 4 0.935 0.927 5 0.964 0.972 6 0.986 0.987 0.990.991 8 0.978 9 0.983 0.981 5本文结论：，我们提出了一个基于潜在向量扩展的自动编码器模型，可以表明我们的方法优于基本的自动编码器结构和异常情况数据的编码器。通过实验表明，该模型在信用卡数据集上的异常情况检测性能优于现有模型。我们将在未来对图像和各种数据集进行实验。表4。BA模型和我们的模型性能比较。Fold BA Ours w/expansion w/expansion AUROC AUROC 1 0.939 0.957 2 0.943 0.969 3 0.884 0.930.851 0.901 5 0.864 0.898 6 0.919 0.945 7 0.790.920.954 0.884 0.910 10 0.847 0.935[1]Y.LeCun，L.Bottou，Y.Bengio和P.Haffner，“基于梯度的学习应用于文件识别”，IEEE会议录，第86卷，第11-2324页，1998年。[2] A.Krizhevsky，G.Hinton等，“从微小图像学习多层特征”，2009年。图2。BA模型和我们的模型PCA比较[3]Y.Park、W.S.Park和Y.B.Kim，“使用假设修剪生成对抗网络的颗粒物传感器异常检测”，ETRI杂志，2020年。[4] W.Sultani，C.Chen和M.Shah，“监控视频中的真实世界异常检测”，载于IEEE计算机视觉和模式识别会议记录，第6479-6488页，2018年。[5] G.E.Hinton，S.Osindero和Y.-W.Teh，“深度信念网络的快速学习算法”，神经计算，第18卷，第7期，第1527-15542006页。[6] D.P.Kingma和M.Welling，“自动编码变量贝叶斯”，arXiv预印本arXiv:1312.61141013。[7] B.Scholkopf，“距离的核心技巧”，神经信息处理系统的进展，第301-307页，2001年。[8] W.S.Noble，“什么是支持向量机？，《自然生物技术》，第24卷，第12期，1565-1567页，2006年。[9] Y.-A.Le Borgne和G.Bontempi，“机器学习用于信用卡欺诈检测实用手册”，ACM SIGKDD探索通讯，第6卷，第1期，第1-6页，2004年。
